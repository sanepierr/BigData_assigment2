{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2451251",
   "metadata": {},
   "source": [
    "# CRISP-DM Pipeline for HousingData\n",
    "\n",
    "This notebook follows the CRISP-DM phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment (summary). Each phase contains descriptive explanations and code implementing common tasks. We use Linear Regression and Random Forest models and evaluate them using RMSE and RÂ²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047dd5f",
   "metadata": {},
   "source": [
    "## 1) Business Understanding\n",
    "\n",
    "The objective is to build regression models to predict the target variable (house price or similar) in the provided `HousingData.xlsx`. I:\n",
    "\n",
    "- Identified the target variable.\n",
    "- Prepared data (handle missing values, encode categoricals, scale if needed).\n",
    "- Trained Linear Regression and Random Forest models.\n",
    "- Compared model performance and discuss results.\n",
    "\n",
    "Scoring: included descriptive explanations at each step for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd098e9c",
   "metadata": {},
   "source": [
    "## 2) Data Understanding\n",
    "\n",
    "Load the dataset and inspect general properties (rows, columns, dtypes, missing values, basic statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2130c23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (506, 15)\n",
      "\n",
      "Columns and dtypes:\n",
      " PID          int64\n",
      "CRIM       float64\n",
      "AC         float64\n",
      "INDUS      float64\n",
      "LS         float64\n",
      "PR         float64\n",
      "RM         float64\n",
      "AGE        float64\n",
      "DIS        float64\n",
      "RAD        float64\n",
      "PTRATIO    float64\n",
      "DMT        float64\n",
      "LSTAT      float64\n",
      "MO         float64\n",
      "TAX        float64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      " PID        0\n",
      "CRIM       2\n",
      "AC         0\n",
      "INDUS      0\n",
      "LS         2\n",
      "PR         3\n",
      "RM         4\n",
      "AGE        4\n",
      "DIS        3\n",
      "RAD        2\n",
      "PTRATIO    3\n",
      "DMT        4\n",
      "LSTAT      1\n",
      "MO         2\n",
      "TAX        1\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>AC</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>LS</th>\n",
       "      <th>PR</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>DMT</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MO</th>\n",
       "      <th>TAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2.0</td>\n",
       "      <td>296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>292.40</td>\n",
       "      <td>4.03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID     CRIM    AC  INDUS   LS     PR     RM   AGE     DIS  RAD  PTRATIO  \\\n",
       "0  101  0.00632  18.0   2.31  0.0  0.538  6.575  65.2  4.0900  1.0     15.3   \n",
       "1  102  0.02731   0.0   7.07  0.0  0.469  6.421  78.9  4.9671  2.0     17.8   \n",
       "2  103  0.02729   0.0   7.07  0.0  0.469  7.185  61.1  4.9671  2.0     17.8   \n",
       "3  104  0.03237   0.0   2.18  0.0  0.458  6.998  45.8  6.0622  3.0     18.7   \n",
       "4  105  0.06905   0.0   2.18  0.0  0.458  7.147  54.2  6.0622  3.0     18.7   \n",
       "\n",
       "      DMT  LSTAT   MO    TAX  \n",
       "0  396.90   4.98  2.0  296.0  \n",
       "1  396.90   9.14  2.0  242.0  \n",
       "2  292.40   4.03  3.0  242.0  \n",
       "3  394.63   2.94  0.0  222.0  \n",
       "4  396.90   5.33  0.0  222.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_excel(r'HousingData.xlsx')\n",
    "print('Shape:', housing.shape)\n",
    "print('\\nColumns and dtypes:\\n', housing.dtypes)\n",
    "print('\\nMissing values per column:\\n', housing.isnull().sum())\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c590f",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "- Look for the likely target column name (commonly 'Price', 'SalePrice', 'price', 'target').\n",
    "- If no obvious target exists, consult the dataset description (or ask instructor). For this notebook we will attempt to automatically pick a plausible numeric target (the largest numeric column by domain knowledge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e31fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected target column: TAX\n"
     ]
    }
   ],
   "source": [
    "# Try to automatically detect a numeric target column (heuristic)\n",
    "num_cols = housing.select_dtypes(include=['number']).columns.tolist()\n",
    "target = None\n",
    "candidates = ['Price','price','SalePrice','saleprice','sale_price','sale_price_usd','target']\n",
    "for c in candidates:\n",
    "    if c in housing.columns:\n",
    "        target = c\n",
    "        break\n",
    "if target is None:\n",
    "    # fallback: choose the numeric column with name suggesting price or, if none, the numeric column with highest variance\n",
    "    for c in num_cols:\n",
    "        if 'price' in c.lower() or 'sale' in c.lower():\n",
    "            target = c\n",
    "            break\n",
    "if target is None and len(num_cols)>0:\n",
    "    target = sorted(num_cols, key=lambda x: housing[x].var() if housing[x].dtype!='object' else 0, reverse=True)[0]\n",
    "\n",
    "print('Detected target column:', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f6b20",
   "metadata": {},
   "source": [
    "## 3) Data Preparation\n",
    "\n",
    "Steps:\n",
    "- Handle missing values (drop or impute).\n",
    "- Encode categorical variables (one-hot or ordinal as appropriate).\n",
    "- Split into train/test.\n",
    "- Feature scaling if needed for linear regression.\n",
    "\n",
    "We provide code that adapts to the dataset automatically but keep explanations in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c63ae0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target: TAX\n",
      "Train shape: (404, 14) Test shape: (102, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Use previously detected target from earlier cell (recompute for robust notebook execution)\n",
    "housing = pd.read_excel(r'HousingData.xlsx')\n",
    "num_cols = housing.select_dtypes(include=['number']).columns.tolist()\n",
    "target = None\n",
    "candidates = ['Price','price','SalePrice','saleprice','sale_price','Sale_Price','target']\n",
    "for c in candidates:\n",
    "    if c in housing.columns:\n",
    "        target = c\n",
    "        break\n",
    "if target is None:\n",
    "    for c in num_cols:\n",
    "        if 'price' in c.lower() or 'sale' in c.lower():\n",
    "            target = c\n",
    "            break\n",
    "if target is None and len(num_cols)>0:\n",
    "    target = sorted(num_cols, key=lambda x: housing[x].var() if housing[x].dtype!='object' else 0, reverse=True)[0]\n",
    "\n",
    "print('Using target:', target)\n",
    "\n",
    "X = housing.drop(columns=[target])\n",
    "y = housing[target].copy()\n",
    "\n",
    "# Basic imputation + encoding for pipeline\n",
    "numeric_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b26c7",
   "metadata": {},
   "source": [
    "## 4) Modeling\n",
    "\n",
    "We'll train two models:\n",
    "- Linear Regression (with preprocessing pipeline)\n",
    "- Random Forest Regressor\n",
    "\n",
    "We'll compare RMSE and RÂ² on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c1339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target: MEDV\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['MEDV'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing target:\u001b[39m\u001b[38;5;124m\"\u001b[39m, target)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Split features and target\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m housing\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[target])\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m housing[target]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Drop rows where target is NaN\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5589\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['MEDV'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Force target column (Boston Housing dataset standard)\n",
    "target = \"MEDV\"\n",
    "print(\"Using target:\", target)\n",
    "\n",
    "# Split features and target\n",
    "X = housing.drop(columns=[target])\n",
    "y = housing[target].copy()\n",
    "\n",
    "# Drop rows where target is NaN\n",
    "mask = y.notna()\n",
    "X = X.loc[mask]\n",
    "y = y.loc[mask]\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "# Pipelines for preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1cd5e",
   "metadata": {},
   "source": [
    "### Model Interpretation & Feature Importance\n",
    "\n",
    "For Random Forest we can extract feature importances. We'll show the top features that contributed to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names after one-hot encoding\n",
    "# This requires fitting the preprocessor separately to get transformed feature names.\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Build feature names list\n",
    "num_features = numeric_features\n",
    "cat_features = []\n",
    "if categorical_features:\n",
    "    # OneHotEncoder inside pipeline -> get categories\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_columns = ohe.get_feature_names_out(categorical_features).tolist()\n",
    "    cat_features = cat_columns\n",
    "\n",
    "feature_names = num_features + cat_features\n",
    "\n",
    "# Get importances from RF\n",
    "rf = rf_pipeline.named_steps['regressor']\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a sorted table\n",
    "import pandas as pd\n",
    "feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feat_imp = feat_imp.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "feat_imp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8b829",
   "metadata": {},
   "source": [
    "## 5) Evaluation\n",
    "\n",
    "Discuss which model performed better and why. Consider:\n",
    "- Underfitting vs overfitting\n",
    "- Effect of non-linear relationships (RF can capture non-linearities)\n",
    "- Importance of features shown above\n",
    "\n",
    "Also consider next steps: hyperparameter tuning, cross-validation, additional feature engineering, or transforming the target (log) if skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62909f4f",
   "metadata": {},
   "source": [
    "## 6) Deployment (summary)\n",
    "\n",
    "Describe how you'd deploy the best model: save the pipeline using `joblib` or `pickle`, create an API endpoint (Flask/FastAPI) that loads the pipeline and accepts new observations, and ensure monitoring and model retraining when performance degrades.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
