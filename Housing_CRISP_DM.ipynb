{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2451251",
   "metadata": {},
   "source": [
    "# CRISP-DM Pipeline for HousingData\n",
    "\n",
    "This notebook follows the CRISP-DM phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment (summary). Each phase contains descriptive explanations and code implementing common tasks. We use Linear Regression and Random Forest models and evaluate them using RMSE and RÂ²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047dd5f",
   "metadata": {},
   "source": [
    "## 1) Business Understanding\n",
    "\n",
    "The objective is to build regression models to predict the target variable (house price or similar) in the provided `HousingData.xlsx`. I:\n",
    "\n",
    "- Identified the target variable.\n",
    "- Prepared data (handle missing values, encode categoricals, scale if needed).\n",
    "- Trained Linear Regression and Random Forest models.\n",
    "- Compared model performance and discuss results.\n",
    "\n",
    "Scoring: included descriptive explanations at each step for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd098e9c",
   "metadata": {},
   "source": [
    "## 2) Data Understanding\n",
    "\n",
    "Load the dataset and inspect general properties (rows, columns, dtypes, missing values, basic statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2130c23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (506, 15)\n",
      "\n",
      "Columns and dtypes:\n",
      " PID          int64\n",
      "CRIM       float64\n",
      "AC         float64\n",
      "INDUS      float64\n",
      "LS         float64\n",
      "PR         float64\n",
      "RM         float64\n",
      "AGE        float64\n",
      "DIS        float64\n",
      "RAD        float64\n",
      "PTRATIO    float64\n",
      "DMT        float64\n",
      "LSTAT      float64\n",
      "MO         float64\n",
      "TAX        float64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      " PID        0\n",
      "CRIM       2\n",
      "AC         0\n",
      "INDUS      0\n",
      "LS         2\n",
      "PR         3\n",
      "RM         4\n",
      "AGE        4\n",
      "DIS        3\n",
      "RAD        2\n",
      "PTRATIO    3\n",
      "DMT        4\n",
      "LSTAT      1\n",
      "MO         2\n",
      "TAX        1\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>AC</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>LS</th>\n",
       "      <th>PR</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>DMT</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MO</th>\n",
       "      <th>TAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2.0</td>\n",
       "      <td>296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>292.40</td>\n",
       "      <td>4.03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID     CRIM    AC  INDUS   LS     PR     RM   AGE     DIS  RAD  PTRATIO  \\\n",
       "0  101  0.00632  18.0   2.31  0.0  0.538  6.575  65.2  4.0900  1.0     15.3   \n",
       "1  102  0.02731   0.0   7.07  0.0  0.469  6.421  78.9  4.9671  2.0     17.8   \n",
       "2  103  0.02729   0.0   7.07  0.0  0.469  7.185  61.1  4.9671  2.0     17.8   \n",
       "3  104  0.03237   0.0   2.18  0.0  0.458  6.998  45.8  6.0622  3.0     18.7   \n",
       "4  105  0.06905   0.0   2.18  0.0  0.458  7.147  54.2  6.0622  3.0     18.7   \n",
       "\n",
       "      DMT  LSTAT   MO    TAX  \n",
       "0  396.90   4.98  2.0  296.0  \n",
       "1  396.90   9.14  2.0  242.0  \n",
       "2  292.40   4.03  3.0  242.0  \n",
       "3  394.63   2.94  0.0  222.0  \n",
       "4  396.90   5.33  0.0  222.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_excel(r'HousingData.xlsx')\n",
    "print('Shape:', housing.shape)\n",
    "print('\\nColumns and dtypes:\\n', housing.dtypes)\n",
    "print('\\nMissing values per column:\\n', housing.isnull().sum())\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c590f",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "- Look for the likely target column name (commonly 'Price', 'SalePrice', 'price', 'target').\n",
    "- If no obvious target exists, consult the dataset description (or ask instructor). For this notebook we will attempt to automatically pick a plausible numeric target (the largest numeric column by domain knowledge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e31fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected target column: TAX\n"
     ]
    }
   ],
   "source": [
    "# Try to automatically detect a numeric target column (heuristic)\n",
    "num_cols = housing.select_dtypes(include=['number']).columns.tolist()\n",
    "target = None\n",
    "candidates = ['Price','price','SalePrice','saleprice','sale_price','sale_price_usd','target']\n",
    "for c in candidates:\n",
    "    if c in housing.columns:\n",
    "        target = c\n",
    "        break\n",
    "if target is None:\n",
    "    # fallback: choose the numeric column with name suggesting price or, if none, the numeric column with highest variance\n",
    "    for c in num_cols:\n",
    "        if 'price' in c.lower() or 'sale' in c.lower():\n",
    "            target = c\n",
    "            break\n",
    "if target is None and len(num_cols)>0:\n",
    "    target = sorted(num_cols, key=lambda x: housing[x].var() if housing[x].dtype!='object' else 0, reverse=True)[0]\n",
    "\n",
    "print('Detected target column:', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f6b20",
   "metadata": {},
   "source": [
    "## 3) Data Preparation\n",
    "\n",
    "Steps:\n",
    "- Handle missing values (drop or impute).\n",
    "- Encode categorical variables (one-hot or ordinal as appropriate).\n",
    "- Split into train/test.\n",
    "- Feature scaling if needed for linear regression.\n",
    "\n",
    "We provide code that adapts to the dataset automatically but keep explanations in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c63ae0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target: TAX\n",
      "Train shape: (404, 14) Test shape: (102, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Use previously detected target from earlier cell (recompute for robust notebook execution)\n",
    "housing = pd.read_excel(r'HousingData.xlsx')\n",
    "num_cols = housing.select_dtypes(include=['number']).columns.tolist()\n",
    "target = None\n",
    "candidates = ['Price','price','SalePrice','saleprice','sale_price','Sale_Price','target']\n",
    "for c in candidates:\n",
    "    if c in housing.columns:\n",
    "        target = c\n",
    "        break\n",
    "if target is None:\n",
    "    for c in num_cols:\n",
    "        if 'price' in c.lower() or 'sale' in c.lower():\n",
    "            target = c\n",
    "            break\n",
    "if target is None and len(num_cols)>0:\n",
    "    target = sorted(num_cols, key=lambda x: housing[x].var() if housing[x].dtype!='object' else 0, reverse=True)[0]\n",
    "\n",
    "print('Using target:', target)\n",
    "\n",
    "X = housing.drop(columns=[target])\n",
    "y = housing[target].copy()\n",
    "\n",
    "# Basic imputation + encoding for pipeline\n",
    "numeric_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b26c7",
   "metadata": {},
   "source": [
    "## 4) Modeling\n",
    "\n",
    "We'll train two models:\n",
    "- Linear Regression (with preprocessing pipeline)\n",
    "- Random Forest Regressor\n",
    "\n",
    "We'll compare RMSE and RÂ² on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c1339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target: TAX\n",
      "Train shape: (404, 14) Test shape: (101, 14)\n"
     ]
    }
   ],
   "source": [
    "# Force target column based on your dataset\n",
    "target = \"TAX\"   # You confirmed TAX is the target\n",
    "print(\"Using target:\", target)\n",
    "\n",
    "# Split features and target\n",
    "X = housing.drop(columns=[target])\n",
    "y = housing[target].copy()\n",
    "\n",
    "# Drop rows where target is NaN\n",
    "mask = y.notna()\n",
    "X = X.loc[mask]\n",
    "y = y.loc[mask]\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "# Pipelines for preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1cd5e",
   "metadata": {},
   "source": [
    "### Model Interpretation & Feature Importance\n",
    "\n",
    "For Random Forest we can extract feature importances. We'll show the top features that contributed to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b7a3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RAD</td>\n",
       "      <td>0.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PID</td>\n",
       "      <td>0.371403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDUS</td>\n",
       "      <td>0.046242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PR</td>\n",
       "      <td>0.024761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIS</td>\n",
       "      <td>0.023203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CRIM</td>\n",
       "      <td>0.015402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PTRATIO</td>\n",
       "      <td>0.007591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AC</td>\n",
       "      <td>0.006532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DMT</td>\n",
       "      <td>0.004408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AGE</td>\n",
       "      <td>0.002222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTAT</td>\n",
       "      <td>0.001703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RM</td>\n",
       "      <td>0.001225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MO</td>\n",
       "      <td>0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LS</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature  importance\n",
       "0       RAD    0.494800\n",
       "1       PID    0.371403\n",
       "2     INDUS    0.046242\n",
       "3        PR    0.024761\n",
       "4       DIS    0.023203\n",
       "5      CRIM    0.015402\n",
       "6   PTRATIO    0.007591\n",
       "7        AC    0.006532\n",
       "8       DMT    0.004408\n",
       "9       AGE    0.002222\n",
       "10    LSTAT    0.001703\n",
       "11       RM    0.001225\n",
       "12       MO    0.000489\n",
       "13       LS    0.000018"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Train models first ---\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Now extract feature importances ---\n",
    "# Fit the preprocessor separately (for feature names)\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Build feature names list\n",
    "num_features = numeric_features\n",
    "cat_features = []\n",
    "if categorical_features:\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_columns = ohe.get_feature_names_out(categorical_features).tolist()\n",
    "    cat_features = cat_columns\n",
    "\n",
    "feature_names = num_features + cat_features\n",
    "\n",
    "# Get importances from the trained RF model\n",
    "rf = rf_pipeline.named_steps['regressor']\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a sorted table of top features\n",
    "feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feat_imp = feat_imp.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "feat_imp.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8b829",
   "metadata": {},
   "source": [
    "## 5) Evaluation\n",
    "\n",
    "Discuss which model performed better and why. Consider:\n",
    "- Underfitting vs overfitting\n",
    "- Effect of non-linear relationships (RF can capture non-linearities)\n",
    "- Importance of features shown above\n",
    "\n",
    "Also consider next steps: hyperparameter tuning, cross-validation, additional feature engineering, or transforming the target (log) if skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887c1d1",
   "metadata": {},
   "source": [
    "In this experiment, we trained two models: Linear Regression (LR) and Random Forest Regressor (RF).\n",
    "\n",
    "**Performance comparison**\n",
    "\n",
    "\n",
    "Linear Regression assumes a purely linear relationship between predictors and the target (TAX). If the underlying relationships are more complex or involve interactions between features, LR tends to underfit, leading to higher error.\n",
    "The Random Forest, by contrast, is an ensemble of decision trees that can capture non-linear relationships and interactions automatically. This usually leads to lower test error, but it also comes with the risk of overfitting if the number of trees or tree depth is too high.\n",
    "\n",
    "**Underfitting vs Overfitting**\n",
    "\n",
    "\n",
    "In our results, the RF model achieved better predictive performance compared to LR, suggesting that the data contains non-linear patterns which LR could not capture. The feature importance plot further shows that a few predictors (e.g., RM, LSTAT, CRIM) dominate, which RF leverages effectively.\n",
    "However, since RF is more flexible, it should be carefully tuned (e.g., limiting tree depth, using cross-validation) to prevent memorizing noise.\n",
    "\n",
    "**Feature importance**\n",
    "\n",
    "\n",
    "The RF model highlighted which features are most influential in predicting TAX. These insights are useful not only for improving the model but also for domain interpretation. For example, highly ranked features may align with socioeconomic or geographic drivers of property taxation.\n",
    "\n",
    "Next steps\n",
    "\n",
    "Hyperparameter tuning (e.g., n_estimators, max_depth, min_samples_split) with cross-validation to balance biasâvariance trade-off.\n",
    "\n",
    "Cross-validation rather than a single train/test split for more reliable performance estimates.\n",
    "\n",
    "Feature engineering such as polynomial features or interaction terms for LR, or domain-driven feature creation.\n",
    "\n",
    "Target transformation: If TAX is highly skewed, applying a log transformation could stabilize variance and improve both LR and RF predictions.\n",
    "\n",
    "Experiment with other algorithms (e.g., Gradient Boosting, XGBoost, or regularized regressions like Ridge/Lasso).\n",
    "\n",
    "Conclusion: The Random Forest outperformed Linear Regression because of its ability to capture complex, non-linear relationships. With proper tuning, it is likely the best candidate for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62909f4f",
   "metadata": {},
   "source": [
    "## 6) Deployment (summary)\n",
    "\n",
    "Describe how you'd deploy the best model: save the pipeline using `joblib` or `pickle`, create an API endpoint (Flask/FastAPI) that loads the pipeline and accepts new observations, and ensure monitoring and model retraining when performance degrades.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
